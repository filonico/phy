# BUILDING A TREE IN A BAYESIAN FRAMEWORK

<br/>
<br/>

## INTRO: 

In this lesson we will use MrBayes [(Ronquist et al., 2012)](https://academic.oup.com/sysbio/article/61/3/539/1674894) to understand the underlying concepts of Bayesian Inference in phylogenetics. Remember that in a Bayesian framework we estimate parameters from their posterior distribution, instead of finding the best single estimates as in a Maximum Likelihood framework.

---

<br/>
<br/>

### SUBSAMPLING OUR DATA:


Unfortunately, next steps of this tutorial will be very computanional intensive, for this reason we need to subsample our 1-to-1 gblocked alignments. We can randomly chose *n* alignments very easly using a for loop and some simple bash commands:

```
mkdir -p Analyses/BI/Alignments
brew install coreutils #For MacOS users
for i in $(ls Analyses/1-to-1_Alignments/gblock | shuf -n 5); do varAL=$( find . -name "$i"); cp "$varAL" Analyses/BI/Alignments/; done
```

---

## MSA FORMAT CONVERSION: 

At this stage we have already used ```.fas``` and ```.nxs``` formats for Multiple Sequence Alignment. 
In this lesson we will also need a ```.phy``` - a phylip-formatted file - which is required by [PartitionFinder2](http://www.robertlanfear.com/partitionfinder/).
This format name comes from a very ancient piece of software - called of course [PHYLIP](https://en.wikipedia.org/wiki/PHYLIP) - which is turning 40 this year :-O
It is a very bare bone format which has in the first line the number of species and the number of characters (nucleotides, proteins, ...) separated by a space and subsequently one entry (whether species or specimen) for each line, with the sequence i.d. as the first element followed by a space and then by the sequence itself.

If you want to play with Bash you can write your own scripts to convert from different formats. Something like:

  from ```.nxs``` to ```.fasta```:

```
awk '/MATRIX/,EOF { print $0 }' concatenation.nxs | tail +2 | tr  \\t ' ' | sed -e 's/^ />/' | tr ' ' '\n' | sed '$d' | sed '$d' | sed '$d' > concatenation.fasta
```

  from ```.nxs``` to ```.phy```:

```
nxs=concatenation.nxs;
awk '/MATRIX/,EOF { print $0 }' $nxs | tail +2 | tr  \\t ' ' | sed 's/^ //g' | sed '$d' | sed '$d' | sed '$d' > tmp.phy; 
n_tax=$(head -4 $nxs | tail -1 | awk -F "NTAX=" '{print $2}' | awk '{print $1}');
n_car=$(head -4 $nxs | tail -1 | awk -F "NCHAR=" '{print $2}' | tr -d ";");
echo $n_tax $n_car > first_line.tmp; cat first_line.tmp tmp.phy > concatenation.phy; 
rm *tmp*
```

But usually, Python it's much more efficient for these operations since it's less prone to bugs and most of the time you will find an already tested package that already do wat you want (saving you time, but if you just want to play with code, you can practice yourself writing your own functions!).

Since we have already installed AMAS we can use again its ```concat``` function but specifyng the phylip output format for the alignment and the Raxml format for the partition file (which will became our ```.cfg``` file):

```
AMAS.py concat -u phylip-int -y raxml -i Analyses/BI/Alignments/OG00000* -f fasta -d aa -p Analyses/BI/My_Partitions.txt -t Analyses/BI/My_concat.phy
```
---

## MODEL SELECTION, AGAIN....

Before the phylogenetic Bayesian Inference (BI), we have to carry out another model selection, mainly for two reasons:

* MrBayes supports slightly different models than ModelFinder (_e.g._ no base frequencies additional parameters)

* even if possible it's really painfull to translate / approximate models from ModelFinder to MrBayes.

We will take advantage of this situation to leverage PartitionFinder2 [(Lanfear et al., 2016)]( https://doi.org/10.1093/molbev/msw260) which can carry out the model selection specifically for MrBayes. This situation teaches us a lesson on usability and how sometimes phylogenetic pipeline consist in a straight process (ModelFinder -> IQ-TREE), but sometimes they result in a quite fragmented workflow with multiple tools and steps in between them.

As previously said the input of PartitionFinder2 are a ```.phy``` and a ```.cfg```.
The latter stands for configuration and is a quite widespread approach to customise analyses while using several pieces of software: all the options are found inside the configuration file, including all the inputs required for the analysis. For convenience, the input alignment is usually placed in the same folder where the configuration file is located, but a path can be specified as well.

For this purpose we have to reformat a bit the our partition file:

```
cat Analyses/BI/My_Partitions.txt | sed 's/^.....//g' | sed 's/$/;/g' > Analyses/BI/partition_finder.cfg
```

Now we have to manually change it following [this](http://www.robertlanfear.com/partitionfinder/tutorial/assets/partition_finder.cfg) pre-formatted configuration. You should already be familiar with many of the options, as they are quite similar to the ones of ModelFinder.
Here is how I configured my ```.cfg``` using ```vim``` :


```
cat Analyses/BI/partition_finder.cfg
## ALIGNMENT FILE ##
alignment = My_concat.phy;

## BRANCHLENGTHS: linked | unlinked ##
branchlengths = linked;

## MODELS OF EVOLUTION: all | allx | mrbayes | beast | gamma | gammai | <list> ##
models = mrbayes;

# MODEL SELECCTION: AIC | AICc | BIC #
model_selection = bic;

## DATA BLOCKS: see manual for how to define ##
[data_blocks]

p1_OG0000031 = 1-151;
p2_OG0000036 = 152-314;
p3_OG0000042 = 315-475;
p4_OG0000061 = 476-805;
p5_OG0000075 = 806-956;

## SCHEMES, search: all | user | greedy | rcluster | rclusterf | kmeans ##
[schemes]

search = rcluster;
```

And run PF specifing the directory where alignments and the cfg file are stored:

```
conda deactivate
conda activate PF-env
python /Users/jacopomartelossi/Desktop/phylogenetic_software/partitionfinder-2.1.1/PartitionFinderProtein.py Analyses/BI/ --raxml --rcluster-max 10
```

The ```--raxml``` flag is necessary to perform the relaxed hierarchical clustering algorithm invocated inside our cfg file and with parameter specified via command-line (```--rcluster-max 10``` option).
Let's have a look at the resulting partitioning scheme:

```
cat Analyses/BI/analysis/best_scheme.txt
```

As you can see this file contains similar informations to ModelFinder. Moreover it includes the MrBayes nexus block (necessary to run it):

```
begin mrbayes;

	charset Subset1 = 152-314 1-151 315-475;
	charset Subset2 = 476-805;
	charset Subset3 = 806-956;

	partition PartitionFinder = 3:Subset1, Subset2, Subset3;
	set partition=PartitionFinder;

	lset applyto=(1) rates=gamma;
	prset applyto=(1) aamodelpr=fixed(wag);
	lset applyto=(2) rates=gamma;
	prset applyto=(2) aamodelpr=fixed(vt);
	lset applyto=(3) rates=gamma;
	prset applyto=(3) aamodelpr=fixed(rtrev);

	prset applyto=(all) ratepr=variable;
	unlink statefreq=(all) revmat=(all) shape=(all) pinvar=(all) tratio=(all);

end;
```

---

<br/>
<br/>

## Bayesian Inference: 

Now we need to include this and other informations in a concatenated alignment in nexus format. Let's re-use AMAS to create a new concatenated alignment (yes again):

```
AMAS.py concat -u nexus -i Analyses/BI/Alignments/OG00000* -f fasta -d aa -t Analyses/BI/My_concat.nex
mv My_concat.nex Analyses/BI/
rm partitions.txt
```

Now let's open the resulting file and add the mrBayes block as follow:

 * **1.** add the MrBayes nexus block from PF results (should be inside ```Analyses/BI/analyses```) **after** the ```END;``` string of the nexus file.
 * **2.** add this line: ```mcmc ngen=500000 printfreq=5000 samplefreq=5000 nruns=2 nchains=4 temp=0.02;``` **after** the last line of the PF results but **before** the ```end``` statement.

This latest string specifies that:

* the analysis must run for **500k generations.**
* every **5k** parameters are printed to standard output.
* every **5k** parameters are sampled.
* **two independent runs** are carried out.
* each run is composed of **4 different chains**
* the temperature of the hot chain is **0.02**

While the first five parameters are quite straightforward, ```temp``` can be a little bit more obscure in the beginning (being somehow similar to the perturbance in IQTREE). The higher the temperature, the more likely the heated chains are to move between isolated peaks in the posterior distribution. However, excessive heating may lead to very low acceptance rates for swaps between different chains.

At the end your mrBayes block should look like this:

```
tail -n 27 Analyses/BI/My_concat.nex
	S_constricta     MFSSKEYLERKTGPYGIGRFSYLQSLVTEYQDTDDLDSKQQVMANLANFAYDPINYDHFRKLNILDIFL-DGLEEQDEKLVEFAVGGICNACLDKCLSRPNEETVLSAITSLMFLCTPGSKSDISCLPVVECMIRFSKAKNKRLSNLATVFMAYSVEKLLSDAAALCSRLKEHDSTADIIISMTQNLHKRIDAMKEYQDDITELNEIAKHRPRSYLVISIAQENRQIRDLQQENRDLRIALDEHQSALELIMKKYREQVNTLIETSKWEDKICEMANVMQ---KSVNVDEDASVADQQRLTQLEIENKGLRELLVCKYLATELKDRLEETGKTKEVLRLGQGLDDSKKRAVSYTTSELRLIEALTDPHICDKILEYNVHKERKGSLRFAKGQSETFKTLHNLVNKGVKVVMDVPYELWNHTPAEVTQLQKHCYQMVSVYEEEIEEWYFKHQ-DQPILDYLCRKHVP--QEERSSVNEVLATETQMFEKGATPSQSNQDLVR-GVNRKRSQSRNNRQMDKSLRLSAEQKCDIAQREIEELREEIDKLKEDSEKVLDTYKAIMEEADLRLAETKKESYEFDRDIMKGALNP-----STNKVVAEKVVRYFDDKIRSRDTLIEKLRLKNSTLKVQKKKLHLQLKQKEEMGEVLHEVDFNQLKIENQQYLEKIDERNQDLLRLKLMAGNTLQVLNSYKKKLHTLTMESERLKSEITSRNDLLTRIDAETSVVEVERAKAEKINKKLRQQLADFRVPEVMEYVSEKADLYELQKKVKSWERKVEIADMALRTHRKTWQQMKISHEMANQWDDELSIPRASLNKMIKEIIPNVRVANDARELILNCCTEFIHLVSTEANEICNKQMKKTISPEHVLAALDSLGFGNYKEDATTVLQEAKAVAAKKRRGSSRLQNLGIPEEELLRQQQELFAQARQEQAQLEQQQWQQMQQALQQQQQQQQQE

;

END;

begin mrbayes;

	charset Subset1 = 152-314 1-151 315-475;
	charset Subset2 = 476-805;
	charset Subset3 = 806-956;

	partition PartitionFinder = 3:Subset1, Subset2, Subset3;
	set partition=PartitionFinder;

	lset applyto=(1) rates=gamma;
	prset applyto=(1) aamodelpr=fixed(wag);
	lset applyto=(2) rates=gamma;
	prset applyto=(2) aamodelpr=fixed(vt);
	lset applyto=(3) rates=gamma;
	prset applyto=(3) aamodelpr=fixed(rtrev);

	prset applyto=(all) ratepr=variable;
	unlink statefreq=(all) revmat=(all) shape=(all) pinvar=(all) tratio=(all);
	mcmc ngen=500000 printfreq=5000 samplefreq=5000 nruns=2 nchains=4 temp=0.02;

end;
```
 
To run MrBayes just type:

```
conda install -c hcc mrbayes #Install mrBayes for MacOS user
conda install -c biobuild mrbayes #Installation for LINUX users
mb -i Analyses/BI/My_concat.nex
```

The program will instantly start writing to the standard output, *e.g.* the initial lnL of the four chains relative to the two runs:

```
      Initial log likelihoods and log prior probs for run 1:
         Chain 1 -- -12453.986041 -- 20.504472
         Chain 2 -- -12538.696635 -- 20.504472
         Chain 3 -- -12455.413128 -- 20.504472
         Chain 4 -- -12349.823398 -- 20.504472

      Initial log likelihoods and log prior probs for run 2:
         Chain 1 -- -12558.784539 -- 20.504472
         Chain 2 -- -12517.162610 -- 20.504472
         Chain 3 -- -12438.758998 -- 20.504472
         Chain 4 -- -12319.557676 -- 20.504472
```

While running, MrBayes will print the chain lnL, which are also written to the .p files. Along with the generation, the average standard deviation of split frequencies are printed: this values are a measure of similarity the tree toplogies sampled by the two independent runs and is highly informative of analysis convergence. It's usually considered that average standard deviation below 0.01 are quite good, values between 0.01 and 0.05 may be adequate depending on the purpose of your analysis, while higher value imply that the analysis is far from stationarity.

```
      Chain results (500000 generations requested):

          0 -- [-12453.986] (-12538.697) (-12455.413) (-12349.823) * [-12558.785] (-12517.163) (-12438.759) (-12319.558) 
       5000 -- [-10134.058] (-10139.817) (-10137.119) (-10137.438) * (-10138.877) (-10144.283) (-10144.509) [-10139.215] -- 2:20:15

      Average standard deviation of split frequencies: 0.265165
```

While running, MrBayes will print the chain lnL, which are also written to the .p files. Along with the generation, the average standard deviation of split frequencies are printed: this values are a measure of similarity the tree toplogies sampled by the two independent runs and is highly informative of analysis convergence. It's usually considered that average standard deviation below 0.01 are quite good, values between 0.01 and 0.05 may be adequate depending on the purpose of your analysis, while higher value imply that the analysis is far from stationarity.

After 100.000 generations i get the following result:

```
      100000 -- (-10135.775) [-10135.630] (-10137.480) (-10137.454) * (-10137.533) [-10136.248] (-10137.057) (-10138.345) -- 1:46:55

      Average standard deviation of split frequencies: 0.047140
```

I will then stop the run before it finishes pressing ```Ctrl-C``` followed by ```q```, as half a million generations are way out of reach with our computational resources. The program has generated several files, including:

* ```.nxs.ckp``` a checkpoint file which let us resume analyses
* two ```.t``` files where are stored all topologies sampled during each run
* two ```.p``` files where are stored all the parameters sampled during each run

Here is how a line from a ```.p``` file looks like:

```
head My_concat.nex.run1.p
[ID: 8287177647]
Gen	LnL	LnPr	TL{all}	alpha{1}	alpha{2}	alpha{3}	m{1}	m{2}	m{3}
0	-1.245591e+04	2.050447e+01	2.600000e-01	1.000000e+00	1.000000e+00	1.000000e+00	1.000000e+00	1.000000e+00	1.000000e+00
5000	-1.013792e+04	-8.209296e+00	2.428374e+00	1.128940e+00	2.763053e+00	7.933907e-01	1.131372e+00	1.090470e+00	3.890263e-01
10000	-1.013617e+04	-7.473407e+00	2.605506e+00	1.024414e+00	1.466974e+00	5.955322e-01	1.198630e+00	9.761592e-01	4.272727e-01
15000	-1.014132e+04	-7.529228e+00	2.642316e+00	8.402696e-01	1.756624e+00	3.738204e-01	1.140685e+00	9.865564e-01	5.868284e-01
20000	-1.013503e+04	-6.970840e+00	2.366228e+00	1.042311e+00	2.325602e+00	3.963275e-01	1.124579e+00	1.016100e+00	5.729270e-01
25000	-1.014055e+04	-7.134557e+00	2.337235e+00	9.626626e-01	2.531080e+00	5.850560e-01	1.133310e+00	1.005999e+00	5.675368e-01
30000	-1.014325e+04	-5.919003e+00	2.322022e+00	9.883992e-01	1.392882e+00	5.618446e-01	1.075673e+00	1.061603e+00	6.273261e-01
35000	-1.013568e+04	-6.720230e+00	2.379092e+00	8.647765e-01	1.888289e+00	6.942183e-01	1.136074e+00	1.037813e+00	4.893164e-01
```

And this is how a ```.t``` file looks like:

```
jacopomartelossi$ head -n 20 My_concat.nex.run1.t
#NEXUS
[ID: 8287177647]
[Param: tree{all}]
begin trees;
   translate
       1 A_granulata,
       2 B_glabrata,
       3 C_virginica,
       4 H_robusta,
       5 L_gigantea,
       6 O_bimaculoides,
       7 P_fucata,
       8 S_constricta;
   tree gen.0 = [&U] (4:2.000000e-02,((8:2.000000e-02,7:2.000000e-02):2.000000e-02,(2:2.000000e-02,((6:2.000000e-02,5:2.000000e-02):2.000000e-02,3:2.000000e-02):2.000000e-02):2.000000e-02):2.000000e-02,1:2.000000e-02);
   tree gen.5000 = [&U] (((2:3.050626e-01,((6:2.876290e-01,4:8.075002e-01):1.267148e-01,(7:1.186048e-01,3:9.942862e-02):9.247354e-02):7.244988e-03):2.312812e-02,5:1.942011e-01):2.408819e-02,8:1.703041e-01,1:1.719938e-01);
   tree gen.10000 = [&U] (2:2.857657e-01,(5:2.022962e-01,(((7:1.390908e-01,3:1.101357e-01):1.119942e-01,(4:8.761623e-01,6:3.018210e-01):1.386016e-01):1.214348e-02,8:1.894397e-01):1.643910e-02):1.058998e-02,1:2.110265e-01);
   tree gen.15000 = [&U] ((8:1.752147e-01,((4:8.843785e-01,6:3.177321e-01):1.573907e-01,5:1.969745e-01):2.423124e-02):5.361995e-03,(2:3.430978e-01,(7:1.450086e-01,3:9.604133e-02):1.004552e-01):1.876186e-02,1:1.776676e-01);
   tree gen.20000 = [&U] ((8:1.906161e-01,((5:1.916084e-01,2:2.449143e-01):1.324881e-02,(6:3.040095e-01,4:7.881834e-01):1.096639e-01):1.201531e-02):1.697782e-02,(7:1.249714e-01,3:1.143852e-01):8.296788e-02,1:1.726658e-01);
   tree gen.25000 = [&U] (((8:1.876950e-01,(7:1.276009e-01,3:8.338116e-02):7.862877e-02):4.856212e-03,(5:1.569944e-01,(6:2.847549e-01,4:7.703767e-01):1.424396e-01):5.089542e-02):2.373391e-02,2:2.772406e-01,1:1.486372e-01);
   tree gen.30000 = [&U] (((2:2.604572e-01,((4:7.712394e-01,6:2.507337e-01):1.118269e-01,5:2.024866e-01):1.825415e-02):1.151876e-02,8:1.401471e-01):8.772548e-03,(7:1.386334e-01,3:1.066647e-01):8.727371e-02,1:2.140143e-01);
```
---

<br/>
<br/>

## Post-Inference diagnostics: 

While there is also plenty of Post-Inference diagnostics approaches which can be applied also to ML analyses, in BI framework they are definitively compulsory. 

We can start by recalling MrBayes with ```mb```. We can then type ```sump Filename = My_concat.nex``` which stands for Summary of Parameters. 
The summary statistics will be calculate using a relative burnin of 0.25 and a written to a ```.nxs.pstat``` file. Nonetheless some interesting information are also printed to the standard output, such as this nice plot of the generation (x-axis) versus the lnL (y-axis). This plot is useful to both decide a sensible burn in for the analysis and to assess the stationarity of the inference. The numbers refer to the two chain and when their value overlap, an asterisk get printed.

```
   Overlay plot for both runs:
   (1 = Run number 1; 2 = Run number 2; * = Both runs)

   +------------------------------------------------------------+ -10132.50
   |                    2                           1       1   |
   |                                                        2   |
   |2   2                           1   2       1       2      1|
   |        1           1               1           2          2|
   |        2   1           *   2           1           1       |
   |                                                            |
   |                1                                           |
   |1           2                                               |
   |                                        2                   |
   |                2               2                           |
   |    1                       1                               |
   |                                                            |
   |                                                            |
   |                                                            |
   |                                            2               |
   +------+-----+-----+-----+-----+-----+-----+-----+-----+-----+ -10148.21
   ^                                                            ^
   25000                                                        100000
   ```
   
Then we have a table which include several values for all the parameters, including the Mean and the Estimated Sample Size (ESS) values. Generally a good run should yield values > 200, but in certain situation (_e.g._ huge trees) values > 150 can be accepted. An additional convergence diagnostic are the Potential scale Reduction Factor (PSRF) values, which should approach 1 as runs converge.

```
   Parameter      Mean      Variance     Lower       Upper       Median    min ESS*  avg ESS    PSRF+ 
   --------------------------------------------------------------------------------------------------
   TL{all}     2.519009    0.012660    2.322022    2.687846    2.491402      4.70     10.35    0.977
   alpha{1}    1.007171    0.011774    0.793939    1.156910    1.037994     12.33     14.16    0.988
   alpha{2}    1.737554    0.092191    1.193971    2.081320    1.732011     16.00     16.00    0.974
   alpha{3}    0.556003    0.010295    0.407015    0.713858    0.561292     16.00     16.00    0.977
   m{1}        1.125977    0.001263    1.051486    1.182968    1.127592     15.75     15.88    0.969
   m{2}        1.055090    0.002332    0.964065    1.125183    1.054173     13.19     14.60    0.968
   m{3}        0.483320    0.004921    0.349353    0.567537    0.487878     15.06     15.53    0.972
   --------------------------------------------------------------------------------------------------
   * Convergence diagnostic (ESS = Estimated Sample Size); min and avg values
     correspond to minimal and average ESS among runs. 
     ESS value below 100 may indicate that the parameter is undersampled. 
   + Convergence diagnostic (PSRF = Potential Scale Reduction Factor; Gelman
     and Rubin, 1992) should approach 1.0 as runs converge.
 ```
 
 As you can see my PSRF are not so bad, but the ESS are very low. To solve this problem we should just run for more time MrBayes.
 
We can then type the ```sumt``` command which - in a similar way to the ```sump``` command - summarises 
the tree and branch length informations. However, before do this you should remove from the input nexus file the line which specifies the parameters of the runs / chains. If you don't do this, the analysis will start again!
 
 ```
 mb -i My_concat.nex
 sumt
 ```
 
 Here are the files which are created:
 
* ```.trprobs``` trees that were found during the MCMC search, sorted by posterior probability
* ```.con.tre```  tree file with clade credibility / posterior probability values
* ```.vstat```    summary statistics for all branch and node parameters
* ```.tstat``` summary statistics for informative (_i.d._ non terminals) taxon bipartitions
* ```.parts``` bipartitions

We can also observe the more important summary statistics from the standard output.

Here there is a depiction of bipartitions: a taxon bipartition is defined by removing a branch on the tree, dividing the tree into those taxa to the left and right of the removed branch. In this representation, dots are the taxa that are on one side of the partition and stars are the taxa on the other side. As you can observe bipartitions 1 to 8 represent terminal branches (they divide the dataset into a subset with one species and another subset with all the other species). while the total number of branches (23) and of internal branches (13). As you can see there are more bipartitions than the 13 branches which we expect ( recall the 2* n. species -3 formula). This is because different trees are sampled throughout the analysis, which have different topologies and a number of possible bipartitions grater than the number of bipartitions in a single tree.

```
   ID -- Partition
   --------------
    1 -- .*******
    2 -- .*......
    3 -- ..*.....
    4 -- ...*....
    5 -- ....*...
    6 -- .....*..
    7 -- ......*.
    8 -- .......*
    9 -- ...*.*..
   10 -- ..*...*.
   11 -- ...***..
   12 -- .*.***.*
   13 -- ..**.**.
   14 -- .*.***..
   15 -- ..******
   16 -- .***.**.
   17 -- ..*...**
   18 -- ..**.***
   19 -- .*.....*
   20 -- .***.***
   21 -- .******.
   22 -- ..*****.
   23 -- .*..*...
   --------------
```
 
This interesting representation can be better interpreted using the conversion table associated:
 
```
      1 -- A_granulata
      2 -- B_glabrata
      3 -- C_virginica
      4 -- H_robusta
      5 -- L_gigantea
      6 -- O_bimaculoides
      7 -- P_fucata
      8 -- S_constricta
 ```
 
You can also find some statistics for informative taxon bipartitions, where ```#obs``` stands for how many times a bipartition has been observed. The ```*``` denote bipartition which were just found in  a single run out of the two.

 ```
   ID   #obs    Probab.     Sd(s)+      Min(s)      Max(s)   Nruns 
   ----------------------------------------------------------------
    9    32    1.000000    0.000000    1.000000    1.000000    2
   10    32    1.000000    0.000000    1.000000    1.000000    2
   11    19    0.593750    0.044194    0.562500    0.625000    2
   12    14    0.437500    0.088388    0.375000    0.500000    2
   13    11    0.343750    0.044194    0.312500    0.375000    2
   14    11    0.343750    0.044194    0.312500    0.375000    2
   15    10    0.312500    0.000000    0.312500    0.312500    2
   16     5    0.156250    0.132583    0.062500    0.250000    2
   17     4    0.125000    0.000000    0.125000    0.125000    2
   18     4    0.125000    0.088388    0.062500    0.187500    2
   19     3    0.093750    0.044194    0.062500    0.125000    2
   20     3    0.093750    0.044194    0.062500    0.125000    2
   21     3    0.093750    0.044194    0.062500    0.125000    2
   22     3    0.093750    0.044194    0.062500    0.125000    2
   23     2    0.062500    0.088388    0.000000    0.125000    1 *
   ----------------------------------------------------------------
   + Convergence diagnostic (standard deviation of split frequencies)
     should approach 0.0 as runs converge.
  ```
  
>This table can also give us a deeper insight into the standard deviation of split frequencies. Also here mb is telling us to leave it running for more time!
  
And here are the summary statistics for branch and node parameters:

```
                                                95% HPD Interval
                                              --------------------
   Parameter           Mean       Variance     Lower       Upper       Median     PSRF+  Nruns
   -------------------------------------------------------------------------------------------
   length{all}[1]     0.177878    0.000317    0.142436    0.202778    0.180471    1.003    2
   length{all}[2]     0.295496    0.000806    0.252990    0.331688    0.297211    0.983    2
   length{all}[3]     0.102429    0.000199    0.079021    0.124927    0.101709    0.980    2
   length{all}[4]     0.828830    0.004128    0.732345    0.939307    0.815754    1.071    2
   length{all}[5]     0.194575    0.000453    0.156994    0.228745    0.199307    1.086    2
   length{all}[6]     0.306774    0.000814    0.250734    0.348142    0.309131    1.029    2
   length{all}[7]     0.142037    0.000238    0.111469    0.158319    0.141960    1.000    2
   length{all}[8]     0.196336    0.000462    0.167399    0.229788    0.197156    0.970    2
   length{all}[9]     0.139222    0.000633    0.097030    0.182737    0.136952    1.026    2
   length{all}[10]    0.083270    0.000152    0.066785    0.101897    0.082392    0.968    2
   length{all}[11]    0.026095    0.000093    0.007929    0.037633    0.024957    0.947    2
   length{all}[12]    0.015295    0.000042    0.006083    0.020172    0.017213    1.175    2
   length{all}[13]    0.015677    0.000108    0.003356    0.030006    0.011971    0.952    2
   length{all}[14]    0.013430    0.000036    0.006801    0.020457    0.011519    0.938    2
   length{all}[15]    0.021434    0.000053    0.011623    0.034721    0.023734    0.900    2
   length{all}[16]    0.014233    0.000014    0.009698    0.018387    0.013141    0.775    2
   length{all}[17]    0.015857    0.000238    0.004856    0.038550    0.012186    1.422    2
   length{all}[18]    0.012223    0.000033    0.005133    0.018239    0.015095     NA      2
   length{all}[19]    0.005780    0.000035    0.002214    0.012581    0.002546     NA      2
   length{all}[20]    0.010387    0.000022    0.005045    0.013433    0.012683    0.577    2
   length{all}[21]    0.015700    0.000108    0.003803    0.022950    0.020347    0.577    2
   length{all}[22]    0.016919    0.000159    0.002704    0.026673    0.021380    0.577    2
   length{all}[23]    0.010685    0.000090    0.003973    0.017397    0.017397     NA      1 *
   -------------------------------------------------------------------------------------------
   + Convergence diagnostic (PSRF = Potential Scale Reduction Factor; Gelman
     and Rubin, 1992) should approach 1.0 as runs converge. NA is reported when
     deviation of parameter values within all runs is 0 or when a parameter
     value (a branch length, for instance) is not sampled in all runs.
   * The partition was not found in all runs so the values are unreliable.
```

At last it is also telling us how may trees sum up to a cumulated probability of 50%, 90% and so on:

```
   Summary statistics for partitions with frequency >= 0.10 in at least one run:
       Average standard deviation of split frequencies = 0.047140
       Maximum standard deviation of split frequencies = 0.132583
       Average PSRF for parameter values (excluding NA and >10.0) = 0.948
       Maximum PSRF for parameter values = 1.422
```

A nicer way to carry out this post-inference diagnostics (which sometimes are also done during the inference) is Tracer,
which offers a much easier way to explore what's going on in our inference. Nonetheless sometimes using the ```sump```and ```sumt```
commands can be faster when working on remote acces servers, as to use Tracer we have to move very large files. 
In Tracer we want to see the famous "fuzzy caterpillar" - by far the most loved animals by phylogeneticists -
which implies that there's no autocorrelation in our analysis and that it has reached stationarity. You can 
take a look  in Tracer at an [awful](https://github.com/for-giobbe/phy/raw/master/examples/bad_trace_example.p) 
and [cool](https://github.com/for-giobbe/phy/raw/master/examples/good_trace_example.p) sampling coming fromt two
distinct runs of MrBayes, to get a sense of how much they can be different.

---

Now let's open our consensus tree and take a look at it! (it will be terrible, you are advise..)


> Probably in many of your trees you will see politomies (*i.e* more than two branches descending from a single node). Remember that MrBayes, as all ML approaches, doesn't produce multifurcating trees during the run! When we have used the ```sumt``` command we have also sumarized all sampled trees in a consensus one (after burn-in discarding). While doing this it has calculated the Posterior Probabilities of each node/bipartition (*i.e* The proportion of the time that the bipartition is found in the sampled trees). If the posterior probability of a node was below the default cutoff of 0.5, mb by default collapsed that branch in a politomy.


## Posterior Probabilities & nodal support overestimation: 

As we have seen before, different metrics of nodal support are associated with different approaches to the phylogenetic inference. BI analyses are associated to Posterior Probabilities - also called clade credibility values - which are calculated from the frequencies  with which they occur in the trees sampled at the stationary state, ranging from 0 to 1. PPs are substantially the proportion of the time  that a bipartition is found and should be considered as an approximation of the posterior probability of the bipartition.

Nonetheless, as we said before, different metrics have different pitfalls and PPs habe been often associated with an overestimation of support especially when data sets with a lot of characters are used and in the presence of small branchlenghts. Generally speaking, standard measures of clade support, such as posterior probabilities and bootstrap proportions, can support several conflicting hypotheses with high support. But this apparent confidence can hinder phylogenetic uncertainty, which can be observed when different metrics are compared and trying to investigate the degree of reproducibility of ou results (different analytical frameworks and inference parameters). You'll find a couple of interesting papers down below!

---


<br/>
<br/>

>If you have lost some results about this lesson you can find them [here](https://github.com/for-giobbe/phy/blob/master/2021/Analyses/Precomputed_BI.tar.gz)

## further reading: 

[Here](http://mrbayes.sourceforge.net/wiki/index.php/Manual) you will find the manual of MrBayes and some tutorials as well.

[Here](http://mrbayes.sourceforge.net/Help/) a very usefull help page for MrBayes.

A couple of nice papers on how the nodal support can be overestimated by [BP](https://doi.org/10.1016/j.ympev.2014.01.018) and [PP](https://doi.org/10.1080/10635150590924208). 
